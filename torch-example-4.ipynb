{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# setting up for AMD chips \n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# setting up for chips Apple silicon\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use `masked_fill` to replace values where `mask` is True with `-inf`\n",
    "# This means all values above the diagonal become `-inf`\n",
    "out = torch.zeros(5,5).masked_fill(torch.tril(torch.ones(5,5)) == 0, float('-inf'))\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# computes the exponential of each element in the input tensor.\n",
    "torch.exp(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a 3D tensor filled with zeros, with shape (2, 3, 4)\n",
    "# Transpose the tensor: swap dimensions 0 and 2\n",
    "input = torch.zeros(2, 3, 4)\n",
    "out = input.transpose(0, 2)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [7, 8, 9]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create three 1D tensors\n",
    "# Stack the tensors along dimension 0 (rows)\n",
    "\n",
    "tensor1 = torch.tensor([1, 2, 3])\n",
    "tensor2 = torch.tensor([4, 5, 6])\n",
    "tensor3 = torch.tensor([7, 8, 9])\n",
    "stacked_tensor = torch.stack((tensor1, tensor2, tensor3), 0)\n",
    "stacked_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.7183, -3.9471,  5.1937], grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "# Create a sample input tensor with three elements\n",
    "# Define a linear layer with input size 3 and output size 3, without bias\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "sample = torch.tensor([10., 10., 10.])\n",
    "linear = nn.Linear(3, 3, bias=False)\n",
    "print(linear(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0900, 0.2447, 0.6652])\n"
     ]
    }
   ],
   "source": [
    "# Create a 1D tensor with three elements\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "tensor1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "softmax_output = F.softmax(tensor1, dim=0)  \n",
    "\n",
    "print(softmax_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 100])\n",
      "tensor([[-0.2342, -0.1424,  0.4821, -0.7892,  0.9484, -0.1277,  1.3946, -1.4655,\n",
      "          0.3696, -0.9044, -0.5586,  0.3756,  0.0928, -0.5791, -0.4634, -0.9792,\n",
      "         -0.4334, -0.0727, -0.9619,  0.3733,  1.3778, -1.4638,  0.7241, -1.1169,\n",
      "          0.2622,  0.8205,  2.2998,  0.6449,  0.3855, -0.6064, -0.2542, -0.3373,\n",
      "          0.4267, -0.6064,  0.5348,  0.5035,  0.3224,  1.0073, -0.2294, -0.3965,\n",
      "          0.5074,  0.0812,  0.0940,  0.2620, -0.2138, -0.9245,  0.8130, -0.9175,\n",
      "         -1.7313, -2.0542,  1.0113, -1.7825, -0.6565,  1.0601, -1.0156, -1.5445,\n",
      "         -1.9709,  1.0898, -1.6185,  0.5036,  0.5700,  0.4504, -0.0212,  1.5864,\n",
      "          0.0214,  1.1140, -0.3475,  0.1621,  1.5676,  0.0864,  0.1103, -0.0956,\n",
      "         -0.0554, -0.5018,  1.2076,  0.1467,  0.4799,  0.4584,  1.2803, -0.8590,\n",
      "         -0.4092, -1.2753,  1.0044, -1.5770,  1.0725,  0.0031, -0.8096,  0.5631,\n",
      "         -0.1233,  0.8747, -2.3007,  0.7647,  0.7230,  0.6739,  0.4575,  1.1258,\n",
      "         -1.4422,  2.6827,  0.6452,  1.1285],\n",
      "        [ 1.0895, -0.0748, -1.1003, -0.4677, -1.3876, -0.1174, -0.7392, -1.4036,\n",
      "         -0.3516,  2.7706, -0.1108,  0.7554,  1.3046, -2.4538,  1.0393, -0.2413,\n",
      "          0.6409,  0.1714, -0.8003, -1.1258, -2.3777,  0.6872, -1.1726, -0.6184,\n",
      "          1.1432,  2.9741,  0.0315, -0.3712, -0.8507,  1.0124,  0.7318, -1.7213,\n",
      "         -1.0377, -0.8063,  0.0168, -0.3148, -0.9377,  1.3868,  1.9547,  0.0524,\n",
      "         -0.4240,  0.9259,  0.6470, -2.3597, -0.2273,  0.1498,  1.6704, -1.0907,\n",
      "         -0.1890,  0.1177,  1.0167, -1.2903, -1.3247,  1.4538,  0.9002,  0.3835,\n",
      "         -0.1789,  0.9305, -0.1749,  2.3090,  0.1665,  1.5276,  1.2725,  0.5868,\n",
      "          0.0066,  0.4523, -1.1737,  0.1818, -0.0110, -1.5738,  1.5206, -0.8129,\n",
      "          0.5985, -0.9652, -1.9629,  0.3286, -0.2850, -0.3364,  1.2692, -0.8217,\n",
      "          0.6055, -2.1104,  2.7640, -1.2593, -2.1329,  1.4207,  1.4337, -1.2434,\n",
      "         -0.4083,  0.7495,  0.3976, -0.0586,  1.2445,  0.8722, -1.0973, -0.0638,\n",
      "         -0.7457, -0.0287,  0.9120, -0.3147],\n",
      "        [ 0.7022, -0.0937,  0.5434, -1.0631,  1.8055, -0.4675, -1.0410, -0.3847,\n",
      "          1.3620,  0.4712, -0.4369, -1.4574,  0.1231, -0.4492,  0.0194,  0.4308,\n",
      "          0.4128,  1.8405, -0.0060,  0.4328,  1.1148, -1.5510,  0.3472, -0.8772,\n",
      "         -1.0775, -0.7712,  0.9972,  1.5038, -0.2392,  0.8494,  1.4173,  0.2667,\n",
      "          0.1552, -0.8385,  2.3011, -0.9565,  0.6880, -0.2935,  0.9424, -1.8966,\n",
      "         -0.8830, -0.5523, -0.4202, -0.5345, -0.4239,  0.8363, -2.2771, -1.0497,\n",
      "          0.5102,  1.4567, -0.5742,  0.2553, -1.1603,  0.9766,  0.8041,  0.1523,\n",
      "          0.9499,  2.2269,  1.6448,  0.6165,  0.1494,  1.6233, -1.5311, -0.4764,\n",
      "         -1.0374,  0.4207, -0.3761, -0.6634, -0.8144, -1.2596, -0.0310, -0.5660,\n",
      "          0.9509,  0.4438,  0.4151,  1.1259, -0.5658, -0.1267,  1.9661, -0.5035,\n",
      "         -0.1597,  0.5920, -0.6899, -0.5675, -0.9922, -0.7639, -1.5916,  1.2995,\n",
      "         -1.3101,  0.5116,  1.1823, -1.2456,  1.3179, -0.7078, -1.2483, -1.7976,\n",
      "          0.3567,  1.7140,  1.1060,  0.4058],\n",
      "        [ 0.3348,  0.0052, -0.1767,  1.1139,  0.7838, -0.4138,  0.2704, -0.5021,\n",
      "         -0.8696, -0.3973,  1.1526, -0.5081,  0.1401, -1.3412, -0.1064,  0.6397,\n",
      "         -0.6608,  0.6242, -1.0622, -0.7092, -1.4008, -0.1351, -1.0285, -0.1384,\n",
      "         -0.2764, -0.5500,  0.8501, -0.3560, -0.5310, -0.7317,  0.6775, -1.6294,\n",
      "          0.1127, -1.2095, -0.0458, -1.6329, -1.1494, -0.3593,  0.7626,  0.5333,\n",
      "          0.9311, -1.6647, -0.0692, -2.0488, -0.9930,  1.0792, -0.5270, -1.3488,\n",
      "         -2.5392, -0.5812, -0.4711, -0.5589, -1.5641, -1.2479, -0.8059, -0.8485,\n",
      "         -0.8440, -0.9118, -2.9303, -1.2306, -0.2899,  0.6467, -0.8329, -0.3266,\n",
      "         -0.7182, -1.6254, -0.6239,  0.2312, -0.7700, -0.2081, -0.7510,  0.4095,\n",
      "          1.3129, -1.1331, -0.0324,  0.8849, -2.4987,  0.4461, -0.3971, -0.8446,\n",
      "          0.5973,  1.5550, -1.5255, -0.5162, -2.7754, -1.9082, -1.5069, -2.6296,\n",
      "         -0.4590,  0.7020, -0.0308,  1.2141,  0.9318, -1.0044, -0.7646,  0.4641,\n",
      "          0.3014, -0.4265, -1.0294, -1.5000]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 1000\n",
    "embeding_dim = 100\n",
    "embedding = nn.Embedding(vocab_size, embeding_dim)\n",
    "\n",
    "input_indices =torch.LongTensor([1,5,3,2])\n",
    "\n",
    "embeded_output = embedding(input_indices)\n",
    "print(embeded_output.shape)\n",
    "print(embeded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 27,  30,  33],\n",
      "        [ 61,  68,  75],\n",
      "        [ 95, 106, 117]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1,2],[3,4],[5,6]])\n",
    "b = torch.tensor([[7,8,9],[10,11,12]])\n",
    "\n",
    "print(torch.matmul(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "int_64 = torch.randint(1, (3,2)).float()\n",
    "\n",
    "float_32 = torch.rand(2,3)\n",
    "\n",
    "result = torch.matmul(int_64, float_32)\n",
    "print(result)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
